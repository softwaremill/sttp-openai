package sttp.openai.requests.responses

import sttp.apispec.Schema
import sttp.openai.json.SnakePickle
import sttp.openai.requests.completions.chat.SchemaSupport
import sttp.openai.requests.completions.chat.message.{Tool, ToolChoice}
import ujson.Value

/** @param background
  *   Whether to run the model response in the background. Learn more.
  * @param createdAt
  *   Unix timestamp (in seconds) of when this Response was created.
  * @param error
  *   An error object returned when the model fails to generate a Response.
  * @param id
  *   Unique identifier for this Response.
  * @param incompleteDetails
  *   Details about why the response is incomplete.
  * @param instructions
  *   A system (or developer) message inserted into the model's context.
  *
  * When using along with previous_response_id, the instructions from a previous response will not be carried over to the next response.
  * This makes it simple to swap out system (or developer) messages in new responses.
  * @param maxOutputTokens
  *   An upper bound for the number of tokens that can be generated for a response, including visible output tokens and reasoning tokens.
  * @param maxToolCalls
  *   The maximum number of total calls to built-in tools that can be processed in a response. This maximum number applies across all
  *   built-in tool calls, not per individual tool. Any further attempts to call a tool by the model will be ignored.
  * @param metadata
  *   Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in
  *   a structured format, and querying for objects via API or the dashboard.
  *
  * Keys are strings with a maximum length of 64 characters. Values are strings with a maximum length of 512 characters.
  * @param model
  *   Model ID used to generate the response, like gpt-4o or o3. OpenAI offers a wide range of models with different capabilities,
  *   performance characteristics, and price points. Refer to the model guide to browse and compare available models.
  * @param `object`
  *   The object type of this resource - always set to response.
  * @param output
  *   An array of content items generated by the model.
  *
  * The length and order of items in the output array is dependent on the model's response. Rather than accessing the first item in the
  * output array and assuming it's an assistant message with the content generated by the model, you might consider using the output_text
  * property where supported in SDKs.
  * @param parallelToolCalls
  *   Whether to allow the model to run tool calls in parallel.
  * @param previousResponseId
  *   The unique ID of the previous response to the model. Use this to create multi-turn conversations. Learn more about conversation state.
  * @param prompt
  *   Reference to a prompt template and its variables. Learn more.
  * @param promptCacheKey
  *   Used by OpenAI to cache responses for similar requests to optimize your cache hit rates. Replaces the user field. Learn more.
  * @param reasoning
  *   o-series models only
  *
  * Configuration options for reasoning models.
  * @param safetyIdentifier
  *   A stable identifier used to help detect users of your application that may be violating OpenAI's usage policies. The IDs should be a
  *   string that uniquely identifies each user. We recommend hashing their username or email address, in order to avoid sending us any
  *   identifying information. Learn more.
  * @param serviceTier
  *   Specifies the processing type used for serving the request.
  *
  * If set to 'auto', then the request will be processed with the service tier configured in the Project settings. Unless otherwise
  * configured, the Project will use 'default'. If set to 'default', then the request will be processed with the standard pricing and
  * performance for the selected model. If set to 'flex' or 'priority', then the request will be processed with the corresponding service
  * tier. Contact sales to learn more about Priority processing. When not set, the default behavior is 'auto'. When the service_tier
  * parameter is set, the response body will include the service_tier value based on the processing mode actually used to serve the request.
  * This response value may be different from the value set in the parameter.
  * @param status
  *   The status of the response generation. One of completed, failed, in_progress, cancelled, queued, or incomplete.
  * @param temperature
  *   What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like
  *   0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both.
  * @param text
  *   Configuration options for a text response from the model. Can be plain text or structured JSON data. Learn more:
  *
  * Text inputs and outputs Structured Outputs
  * @param toolChoice
  *   How the model should select which tool (or tools) to use when generating a response. See the tools parameter to see how to specify
  *   which tools the model can call.
  * @param tools
  *   An array of tools the model may call while generating a response. You can specify which tool to use by setting the tool_choice
  *   parameter.
  *
  * The two categories of tools you can provide the model are:
  *
  * Built-in tools: Tools that are provided by OpenAI that extend the model's capabilities, like web search or file search. Learn more about
  * built-in tools. Function calls (custom tools): Functions that are defined by you, enabling the model to call your own code. Learn more
  * about function calling.
  * @param topLogprobs
  *   An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log
  *   probability.
  * @param topP
  *   An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p
  *   probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.
  *
  * We generally recommend altering this or temperature but not both.
  * @param truncation
  *   The truncation strategy to use for the model response.
  *
  * auto: If the context of this response and previous ones exceeds the model's context window size, the model will truncate the response to
  * fit the context window by dropping input items in the middle of the conversation. disabled (default): If a model response will exceed
  * the context window size for a model, the request will fail with a 400 error.
  * @param usage
  *   Represents token usage details including input tokens, output tokens, a breakdown of output tokens, and the total tokens used.
  * @param user
  *   Deprecated This field is being replaced by safety_identifier and prompt_cache_key. Use prompt_cache_key instead to maintain caching
  *   optimizations. A stable identifier for your end-users. Used to boost cache hit rates by better bucketing similar requests and to help
  *   OpenAI detect and prevent abuse. Learn more.
  */
case class ResponsesResponseBody(
    background: Option[Boolean] = None,
    createdAt: Long,
    error: Option[ResponsesResponseBody.ErrorObject] = None,
    id: String,
    incompleteDetails: Option[ResponsesResponseBody.IncompleteDetails] = None,
    instructions: Option[Either[String, List[String]]] = None,
    maxOutputTokens: Option[Int] = None,
    maxToolCalls: Option[Int] = None,
    metadata: Option[Map[String, String]] = None,
    model: String,
    `object`: String,
    output: List[ResponsesResponseBody.OutputItem],
    parallelToolCalls: Option[Boolean] = None,
    previousResponseId: Option[String] = None,
    prompt: Option[ResponsesResponseBody.PromptConfig] = None,
    promptCacheKey: Option[String] = None,
    reasoning: Option[ResponsesResponseBody.ReasoningConfig] = None,
    safetyIdentifier: Option[String] = None,
    serviceTier: Option[String] = None,
    status: String,
    temperature: Option[Double] = None,
    text: Option[ResponsesResponseBody.TextConfig] = None,
    toolChoice: Option[ToolChoice] = None,
    tools: Option[List[Tool]] = None,
    topLogprobs: Option[Int] = None,
    topP: Option[Double] = None,
    truncation: Option[String] = None,
    usage: Option[ResponsesResponseBody.Usage] = None,
    user: Option[String] = None
)

object ResponsesResponseBody {

  case class ErrorObject(
      code: Option[String] = None,
      message: String,
      param: Option[String] = None,
      `type`: Option[String] = None
  )

  case class IncompleteDetails(
      reason: String
  )

  /** @param id
    *   The unique identifier of the prompt template to use.
    * @param variables
    *   Optional map of values to substitute in for variables in your prompt. The substitution values can either be strings, or other
    *   Response input types like images or files.
    * @param version
    *   Optional version of the prompt template.
    */
  case class PromptConfig(
      id: String,
      variables: Option[Map[String, String]] = None,
      version: Option[String] = None
  )

  /** @param effort
    *   o-series models only
    *
    * Constrains effort on reasoning for reasoning models. Currently supported values are low, medium, and high. Reducing reasoning effort
    * can result in faster responses and fewer tokens used on reasoning in a response.
    * @param generateSummary
    *   Deprecated: use summary instead.
    *
    * A summary of the reasoning performed by the model. This can be useful for debugging and understanding the model's reasoning process.
    * One of auto, concise, or detailed.
    * @param summary
    *   A summary of the reasoning performed by the model. This can be useful for debugging and understanding the model's reasoning process.
    *   One of auto, concise, or detailed.
    */
  case class ReasoningConfig(
      effort: Option[String] = None,
      generateSummary: Option[String] = None,
      summary: Option[String] = None
  )

  sealed trait OutputItem
  object OutputItem {

    @upickle.implicits.key("message")
    case class OutputMessage(
        content: List[OutputContent],
        id: String,
        role: String,
        status: String
    ) extends OutputItem

    @upickle.implicits.key("file_search_call")
    case class FileSearchToolCall(
        id: String,
        queries: List[String],
        status: String,
        results: Option[List[FileSearchResult]] = None
    ) extends OutputItem

    @upickle.implicits.key("function_call")
    case class FunctionToolCall(
        arguments: String,
        callId: String,
        name: String,
        id: String,
        status: String
    ) extends OutputItem

    @upickle.implicits.key("web_search_call")
    case class WebSearchToolCall(
        action: Value,
        id: String,
        status: String
    ) extends OutputItem

    @upickle.implicits.key("computer_call")
    case class ComputerToolCall(
        action: Value,
        callId: String,
        id: String,
        pendingSafetyChecks: List[PendingSafetyCheck],
        status: String
    ) extends OutputItem

    @upickle.implicits.key("reasoning")
    case class Reasoning(
        id: String,
        summary: List[SummaryText],
        encryptedContent: Option[String] = None,
        status: Option[String] = None
    ) extends OutputItem

    @upickle.implicits.key("image_generation_call")
    case class ImageGenerationCall(
        id: String,
        result: Option[String],
        status: String
    ) extends OutputItem

    @upickle.implicits.key("code_interpreter_call")
    case class CodeInterpreterToolCall(
        code: Option[String],
        containerId: String,
        id: String,
        outputs: Option[List[CodeInterpreterOutput]] = None,
        status: String
    ) extends OutputItem

    @upickle.implicits.key("local_shell_call")
    case class LocalShellCall(
        action: LocalShellAction,
        callId: String,
        id: String,
        status: String
    ) extends OutputItem

    @upickle.implicits.key("mcp_call")
    case class McpToolCall(
        arguments: String,
        id: String,
        name: String,
        serverLabel: String,
        error: Option[String] = None,
        output: Option[String] = None
    ) extends OutputItem

    @upickle.implicits.key("mcp_list_tools")
    case class McpListTools(
        id: String,
        serverLabel: String,
        tools: List[Value],
        error: Option[String] = None
    ) extends OutputItem

    @upickle.implicits.key("mcp_approval_request")
    case class McpApprovalRequest(
        arguments: String,
        id: String,
        name: String,
        serverLabel: String
    ) extends OutputItem

    // Helper types
    case class PendingSafetyCheck(
        code: String,
        id: String,
        message: String,
        status: String
    )

    case class SummaryText(
        text: String
    )

    case class FileSearchResult(
        attributes: Map[String, Value],
        fileId: String,
        filename: String,
        score: Double,
        text: String
    )

    case class LocalShellAction(
        command: List[String],
        env: Option[Map[String, String]] = None,
        `type`: String,
        timeoutMs: Option[Int] = None,
        user: Option[String] = None,
        workingDirectory: Option[String] = None
    )

    sealed trait CodeInterpreterOutput
    object CodeInterpreterOutput {
      @upickle.implicits.key("logs")
      case class Logs(logs: String) extends CodeInterpreterOutput

      @upickle.implicits.key("image")
      case class Image(image: String) extends CodeInterpreterOutput

      implicit val logsRW: SnakePickle.Reader[Logs] = SnakePickle.macroR
      implicit val imageRW: SnakePickle.Reader[Image] = SnakePickle.macroR
      implicit val codeInterpreterOutputRW: SnakePickle.Reader[CodeInterpreterOutput] = SnakePickle.macroR
    }

    // Implicit readers for all output item types
    implicit val pendingSafetyCheckRW: SnakePickle.Reader[PendingSafetyCheck] = SnakePickle.macroR
    implicit val summaryTextRW: SnakePickle.Reader[SummaryText] = SnakePickle.macroR
    implicit val fileSearchResultRW: SnakePickle.Reader[FileSearchResult] = SnakePickle.macroR
    implicit val localShellActionRW: SnakePickle.Reader[LocalShellAction] = SnakePickle.macroR

    implicit val outputMessageRW: SnakePickle.Reader[OutputMessage] = SnakePickle.macroR
    implicit val fileSearchToolCallRW: SnakePickle.Reader[FileSearchToolCall] = SnakePickle.macroR
    implicit val functionToolCallRW: SnakePickle.Reader[FunctionToolCall] = SnakePickle.macroR
    implicit val webSearchToolCallRW: SnakePickle.Reader[WebSearchToolCall] = SnakePickle.macroR
    implicit val computerToolCallRW: SnakePickle.Reader[ComputerToolCall] = SnakePickle.macroR
    implicit val reasoningRW: SnakePickle.Reader[Reasoning] = SnakePickle.macroR
    implicit val imageGenerationCallRW: SnakePickle.Reader[ImageGenerationCall] = SnakePickle.macroR
    implicit val codeInterpreterToolCallRW: SnakePickle.Reader[CodeInterpreterToolCall] = SnakePickle.macroR
    implicit val localShellCallRW: SnakePickle.Reader[LocalShellCall] = SnakePickle.macroR
    implicit val mcpToolCallRW: SnakePickle.Reader[McpToolCall] = SnakePickle.macroR
    implicit val mcpListToolsRW: SnakePickle.Reader[McpListTools] = SnakePickle.macroR
    implicit val mcpApprovalRequestRW: SnakePickle.Reader[McpApprovalRequest] = SnakePickle.macroR

    implicit val outputItemRW: SnakePickle.Reader[OutputItem] = SnakePickle.macroR
  }

  sealed trait OutputContent
  object OutputContent {
    @upickle.implicits.key("output_text")
    case class OutputText(
        annotations: List[Annotation],
        text: String,
        logprobs: Option[List[LogProb]] = None
    ) extends OutputContent

    @upickle.implicits.key("refusal")
    case class Refusal(
        refusal: String
    ) extends OutputContent

    sealed trait Annotation
    object Annotation {
      @upickle.implicits.key("file_citation")
      case class FileCitation(fileId: String, filename: String, index: Int) extends Annotation

      @upickle.implicits.key("url_citation")
      case class UrlCitation(endIndex: Int, startIndex: Int, title: String, url: String) extends Annotation

      @upickle.implicits.key("container_file_citation")
      case class ContainerFileCitation(containerId: String, endIndex: Int, fileId: String, filename: String, startIndex: Int)
          extends Annotation

      @upickle.implicits.key("file_path")
      case class FilePath(fileId: String, index: Int) extends Annotation

      implicit val fileCitationRW: SnakePickle.Reader[FileCitation] = SnakePickle.macroR
      implicit val urlCitationRW: SnakePickle.Reader[UrlCitation] = SnakePickle.macroR
      implicit val containerFileCitationRW: SnakePickle.Reader[ContainerFileCitation] = SnakePickle.macroR
      implicit val filePathRW: SnakePickle.Reader[FilePath] = SnakePickle.macroR
      implicit val annotationRW: SnakePickle.Reader[Annotation] = SnakePickle.macroR
    }

    case class LogProb(bytes: List[Byte], logprob: Double, token: String, topLogprobs: List[TopLogProb])
    case class TopLogProb(bytes: List[Byte], logprob: Double, token: String)

    implicit val topLogProbRW: SnakePickle.Reader[TopLogProb] = SnakePickle.macroR
    implicit val logProbRW: SnakePickle.Reader[LogProb] = SnakePickle.macroR
    implicit val outputTextRW: SnakePickle.Reader[OutputText] = SnakePickle.macroR
    implicit val refusalRW: SnakePickle.Reader[Refusal] = SnakePickle.macroR
    implicit val outputContentRW: SnakePickle.Reader[OutputContent] = SnakePickle.macroR
  }

  sealed trait Format
  object Format {

    /** Default response format. Used to generate text responses.
      *
      * The type of response format being defined. Always text.
      */
    case object Text extends Format

    /** JSON object response format. An older method of generating JSON responses. Using json_schema is recommended for models that support
      * it. Note that the model will not generate JSON without a system or user message instructing it to do so.
      *
      * The type of response format being defined. Always json_object.
      */
    case object JsonObject extends Format

    /** JSON Schema response format. Used to generate structured JSON responses. Learn more about Structured Outputs.
      *
      * @param name
      *   The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64.
      * @param strict
      *   Whether to enable strict schema adherence when generating the output. If set to true, the model will always follow the exact
      *   schema defined in the schema field. Only a subset of JSON Schema is supported when strict is true. To learn more, read the
      *   Structured Outputs guide.
      * @param schema
      *   The schema for the response format, described as a JSON Schema object. Learn how to build JSON schemas here.
      * @param description
      *   A description of what the response format is for, used by the model to determine how to respond in the format.
      */
    @upickle.implicits.key("json_schema")
    case class JsonSchema(name: String, strict: Option[Boolean], schema: Option[Schema], description: Option[String]) extends Format

    implicit val schemaRW: SnakePickle.Reader[Schema] = SchemaSupport.schemaRW

    implicit val textRW: SnakePickle.Reader[Text.type] = SnakePickle
      .reader[ujson.Value]
      .map(_ => Text)

    implicit val jsonObjectRW: SnakePickle.Reader[JsonObject.type] = SnakePickle
      .reader[ujson.Value]
      .map(_ => JsonObject)

    implicit val jsonSchemaRW: SnakePickle.Reader[JsonSchema] = SnakePickle.macroR

    implicit val formatRW: SnakePickle.Reader[Format] = SnakePickle
      .reader[ujson.Value]
      .map(json =>
        json.obj.get("type").map(_.str) match {
          case Some("text")        => Text
          case Some("json_object") => JsonObject
          case Some("json_schema") => SnakePickle.read[JsonSchema](json)
          case other               => throw new RuntimeException(s"Unknown format type: $other")
        }
      )
  }

  /** @param format
    *   An object specifying the format that the model must output.
    *
    * Configuring { "type": "json_schema" } enables Structured Outputs, which ensures the model will match your supplied JSON schema. Learn
    * more in the Structured Outputs guide.
    *
    * The default format is { "type": "text" } with no additional options.
    *
    * Not recommended for gpt-4o and newer models:
    *
    * Setting to { "type": "json_object" } enables the older JSON mode, which ensures the message the model generates is valid JSON. Using
    * json_schema is preferred for models that support it.
    */
  case class TextConfig(
      format: Option[Format] = None
  )

  /** @param inputTokens
    *   The number of input tokens.
    * @param outputTokens
    *   The number of output tokens.
    * @param totalTokens
    *   The total number of tokens used.
    * @param inputTokensDetails
    *   A detailed breakdown of the input tokens.
    * @param outputTokensDetails
    *   A detailed breakdown of the output tokens.
    */
  case class Usage(
      inputTokens: Int,
      outputTokens: Int,
      totalTokens: Int,
      inputTokensDetails: Option[InputTokensDetails] = None,
      outputTokensDetails: Option[OutputTokensDetails] = None
  )

  /** @param cachedTokens
    *   The number of tokens that were retrieved from the cache. More on prompt caching.
    */
  case class InputTokensDetails(
      cachedTokens: Int
  )

  /** @param reasoningTokens
    *   Tokens generated by the model for reasoning.
    */
  case class OutputTokensDetails(
      reasoningTokens: Option[Int] = None
  )

  implicit val inputTokensDetailsRW: SnakePickle.Reader[InputTokensDetails] = SnakePickle.macroR
  implicit val outputTokensDetailsRW: SnakePickle.Reader[OutputTokensDetails] = SnakePickle.macroR
  implicit val usageRW: SnakePickle.Reader[Usage] = SnakePickle.macroR

  implicit val instructionsRW: SnakePickle.Reader[Either[String, List[String]]] = SnakePickle
    .reader[ujson.Value]
    .map {
      case ujson.Str(str) => Left(str)
      case ujson.Arr(arr) => Right(arr.map(_.str).toList)
      case json           => throw new RuntimeException(s"Expected string or array for instructions, got: $json")
    }

  implicit val promptConfigRW: SnakePickle.Reader[PromptConfig] = SnakePickle.macroR
  implicit val reasoningConfigRW: SnakePickle.Reader[ReasoningConfig] = SnakePickle.macroR
  implicit val textConfigRW: SnakePickle.Reader[TextConfig] = SnakePickle.macroR
  implicit val errorObjectRW: SnakePickle.Reader[ErrorObject] = SnakePickle.macroR
  implicit val incompleteDetailsRW: SnakePickle.Reader[IncompleteDetails] = SnakePickle.macroR
  implicit val responsesResponseBodyRW: SnakePickle.Reader[ResponsesResponseBody] = SnakePickle.macroR
}
